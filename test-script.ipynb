{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip install tez","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:06.293574Z","iopub.execute_input":"2022-09-10T17:34:06.294005Z","iopub.status.idle":"2022-09-10T17:34:06.301721Z","shell.execute_reply.started":"2022-09-10T17:34:06.293891Z","shell.execute_reply":"2022-09-10T17:34:06.300931Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.enable()\n\n\n\nimport os\nfrom IPython.core.display import display, HTML\nimport numpy as np\nimport pandas as pd\nimport tez\nimport torch\nimport torch.nn as nn\nfrom joblib import Parallel, delayed\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer\nfrom torch.utils.data import Dataset,DataLoader\nimport glob\nfrom timeit import default_timer as timer\nfrom torch.utils.data.sampler import *\nimport torch.cuda.amp as amp\nfrom torch.nn.parallel.data_parallel import data_parallel\n\nimport os\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"","metadata":{"_uuid":"66c995cf-7cbb-421b-bdd3-24906f5fb886","_cell_guid":"e0e4bdf6-bcd9-4f82-a53b-9c5f685500a8","collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2022-09-10T17:39:30.884603Z","iopub.execute_input":"2022-09-10T17:39:30.884879Z","iopub.status.idle":"2022-09-10T17:39:30.891561Z","shell.execute_reply.started":"2022-09-10T17:39:30.884847Z","shell.execute_reply":"2022-09-10T17:39:30.890553Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def time_to_str(t, mode='min'):\n    if mode=='min':\n        t  = int(t)/60\n        hr = t//60\n        min = t%60\n        return '%2d hr %02d min'%(hr,min)\n    elif mode=='sec':\n        t   = int(t)\n        min = t//60\n        sec = t%60\n        return '%2d min %02d sec'%(min,sec)\n    else:\n        raise NotImplementedError","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:08.506508Z","iopub.execute_input":"2022-09-10T17:34:08.506756Z","iopub.status.idle":"2022-09-10T17:34:08.513780Z","shell.execute_reply.started":"2022-09-10T17:34:08.506717Z","shell.execute_reply":"2022-09-10T17:34:08.512926Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"discourse_marker_to_label = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 2,\n    \"I-Position\": 3,\n    \"B-Evidence\": 4,\n    \"I-Evidence\": 5,\n    \"B-Claim\": 6,\n    \"I-Claim\": 7,\n    \"B-Concluding Statement\": 8,\n    \"I-Concluding Statement\": 9,\n    \"B-Counterclaim\": 10,\n    \"I-Counterclaim\": 11,\n    \"B-Rebuttal\": 12,\n    \"I-Rebuttal\": 13,\n    \"O\": 14,\n    \"PAD\": -100,\n}\n\n\nlabel_to_discourse_marker = {v: k for k, v in discourse_marker_to_label.items()}\n\nnum_discourse_marker = 15 \n\nprobability_threshold = {\n    \"Lead\": 0.687,\n    \"Position\": 0.537,\n    \"Evidence\": 0.637,\n    \"Claim\": 0.537,\n    \"Concluding Statement\": 0.687,\n    \"Counterclaim\": 0.537,\n    \"Rebuttal\": 0.537,\n \n}\n\nlength_threshold = {\n    \"Lead\": 7,\n    \"Position\": 3,\n    \"Evidence\": 12,\n    \"Claim\": 1,\n    \"Concluding Statement\": 9,\n    \"Counterclaim\": 4,\n    \"Rebuttal\": 2,\n}\nmax_length = 1600\nis_amp = True","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:08.515093Z","iopub.execute_input":"2022-09-10T17:34:08.515489Z","iopub.status.idle":"2022-09-10T17:34:08.527065Z","shell.execute_reply.started":"2022-09-10T17:34:08.515453Z","shell.execute_reply":"2022-09-10T17:34:08.526229Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"model_grps = {\n\n\n    \n    'deberta':{\n        'arch' : '../input/debertalarge',\n        'batch_size':8,\n        'max_length' : max_length,\n        'checkpoints':\n        [\n            ('../input/deberta-large-5-fold/model_0.bin','deberta_fold_0'),\n\n        ]    \n    },\n    \n}","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:08.529602Z","iopub.execute_input":"2022-09-10T17:34:08.530123Z","iopub.status.idle":"2022-09-10T17:34:08.540543Z","shell.execute_reply.started":"2022-09-10T17:34:08.530084Z","shell.execute_reply":"2022-09-10T17:34:08.539737Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def tokenize_data(texts, tokenizer, max_length = 4096):\n    results = []\n\n    for id_, text in texts:\n        encoded = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n            max_length=1600,  # <todo>\n            truncation=True,\n        )\n        token_id = encoded['input_ids']\n        token_offset = encoded['offset_mapping']\n\n        # add end, start token id\n        token_id = [tokenizer.cls_token_id] + token_id\n        token_id = token_id[: max_length - 1]  # need to set as 4096, do not abandon tokens\n        token_id = token_id + [tokenizer.sep_token_id]\n\n        # padding\n        token_mask = [1] * len(token_id)\n\n        #     padding_length = max_length - len(token_id)\n        #     if padding_length > 0:\n        #         if tokenizer.padding_side == 'right':\n        #             token_id    = token_id    + [tokenizer.pad_token_id] * padding_length\n        #             token_mask  = token_mask  + [0] * padding_length\n        #         else:\n        #             raise NotImplementedError\n\n        results.append((id_, text, token_offset, token_id, token_mask))\n\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:08.542081Z","iopub.execute_input":"2022-09-10T17:34:08.542376Z","iopub.status.idle":"2022-09-10T17:34:08.553591Z","shell.execute_reply.started":"2022-09-10T17:34:08.542338Z","shell.execute_reply":"2022-09-10T17:34:08.552915Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class FeedbackDataset(Dataset):\n    def __init__(self, tokenized_data):\n        self.tokenized_data = tokenized_data\n        self.length = len(self.tokenized_data)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        # text to token\n\n        id, text, token_offset, token_id, token_mask = self.tokenized_data[index]\n\n        # -------------------------------------\n        r = {}\n        r['index'] = index\n        r['id'] = id\n        r['text'] = text\n        r['token_offset'] = str(token_offset)  # force batch loader store as list\n        #         r['token_id'    ] = torch.tensor(token_id,    dtype=torch.long)\n        #         r['token_mask'  ] = torch.tensor(token_mask,  dtype=torch.long)\n        r['token_id'] = token_id\n        r['token_mask'] = token_mask\n\n        return r\n\n\nclass FeedbackDatasetValid:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n\n        # add end token id to the input_ids\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n\n        return {\n            \"ids\": input_ids,\n            \"mask\": attention_mask,\n        }\n\n\nclass Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"token_id\"] = [sample[\"token_id\"] for sample in batch]\n        output[\"token_mask\"] = [sample[\"token_mask\"] for sample in batch]\n        output[\"index\"] = [sample['index'] for sample in batch]\n        output[\"id\"] = [sample[\"id\"] for sample in batch]\n        output[\"text\"] = [sample[\"text\"] for sample in batch]\n        output[\"token_offset\"] = [sample[\"token_offset\"] for sample in batch]\n\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"token_id\"]])\n        # batch_max = 4096\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"token_id\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"token_id\"]]\n            output[\"token_mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"token_mask\"]]\n        else:\n            output[\"token_id\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"token_id\"]]\n            output[\"token_mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"token_mask\"]]\n\n        # convert to tensors\n        output[\"token_id\"] = torch.tensor(output[\"token_id\"], dtype=torch.long)\n        output[\"token_mask\"] = torch.tensor(output[\"token_mask\"], dtype=torch.long)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:08.555672Z","iopub.execute_input":"2022-09-10T17:34:08.555883Z","iopub.status.idle":"2022-09-10T17:34:08.573918Z","shell.execute_reply.started":"2022-09-10T17:34:08.555852Z","shell.execute_reply":"2022-09-10T17:34:08.573214Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"valid_id = [ f.replace(\"\\\\\", \"/\").split('/')[-1][:-4] for f in glob.glob('../input/feedback-prize-2021/test'+'/*.txt') ] # get file names\n# valid_id = pd.read_csv('../input/feedback-prize-2021/train.csv').id.unique().tolist()\nvalid_id = sorted(valid_id)\nnum_valid = len(valid_id)\nprint('len(valid_id)',len(valid_id))","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:08.575173Z","iopub.execute_input":"2022-09-10T17:34:08.575595Z","iopub.status.idle":"2022-09-10T17:34:08.593153Z","shell.execute_reply.started":"2022-09-10T17:34:08.575560Z","shell.execute_reply":"2022-09-10T17:34:08.592407Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"len(valid_id) 5\n","output_type":"stream"}]},{"cell_type":"code","source":"df_text=[]\nfor id in valid_id:\n    text_file = '../input/feedback-prize-2021/test' +'/%s.txt'%id\n    with open(text_file, 'r') as f:\n        text = f.read()\n\n    text = text.replace(u'\\xa0', u' ')\n    text = text.rstrip()\n    text = text.lstrip()\n    df_text.append((id,text))\ndf_text = pd.DataFrame(df_text, columns=['id','text'])\ndf_text['text_len'] = df_text['text'].apply(lambda x: len(x))\ndf_text = df_text.sort_values('text_len',ascending=False).reset_index(drop=True)\ndel df_text['text_len']\n\nprint('df_text.shape',df_text.shape)\nprint(df_text) # sort txt by its length","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:08.594379Z","iopub.execute_input":"2022-09-10T17:34:08.595244Z","iopub.status.idle":"2022-09-10T17:34:08.644403Z","shell.execute_reply.started":"2022-09-10T17:34:08.595215Z","shell.execute_reply":"2022-09-10T17:34:08.643618Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"df_text.shape (5, 2)\n             id                                               text\n0  18409261F5C2  80% of Americans believe seeking multiple opin...\n1  0FB0700DAF44  During a group project, have you ever asked a ...\n2  DF920E0A7337  Have you ever asked more than one person for h...\n3  D72CB1C11673  Making choices in life can be very difficult. ...\n4  D46BCB48440A  When people ask for advice,they sometimes talk...\n","output_type":"stream"}]},{"cell_type":"code","source":"df_text","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:08.650160Z","iopub.execute_input":"2022-09-10T17:34:08.652293Z","iopub.status.idle":"2022-09-10T17:34:08.672707Z","shell.execute_reply.started":"2022-09-10T17:34:08.652255Z","shell.execute_reply":"2022-09-10T17:34:08.671466Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"             id                                               text\n0  18409261F5C2  80% of Americans believe seeking multiple opin...\n1  0FB0700DAF44  During a group project, have you ever asked a ...\n2  DF920E0A7337  Have you ever asked more than one person for h...\n3  D72CB1C11673  Making choices in life can be very difficult. ...\n4  D46BCB48440A  When people ask for advice,they sometimes talk...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>18409261F5C2</td>\n      <td>80% of Americans believe seeking multiple opin...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0FB0700DAF44</td>\n      <td>During a group project, have you ever asked a ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>DF920E0A7337</td>\n      <td>Have you ever asked more than one person for h...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>D72CB1C11673</td>\n      <td>Making choices in life can be very difficult. ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>D46BCB48440A</td>\n      <td>When people ask for advice,they sometimes talk...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def text_to_word(text):\n    word = text.split()\n    word_offset = []\n\n    start = 0\n    for w in word:\n        r = text[start:].find(w)\n\n        if r==-1:\n            raise NotImplementedError\n        else:\n            start = start+r\n            end   = start+len(w)\n            word_offset.append((start,end))\n            #print('%32s'%w, '%5d'%start, '%5d'%r, text[start:end])\n        start = end\n\n    return word, word_offset","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:08.678786Z","iopub.execute_input":"2022-09-10T17:34:08.681109Z","iopub.status.idle":"2022-09-10T17:34:08.690609Z","shell.execute_reply.started":"2022-09-10T17:34:08.681078Z","shell.execute_reply":"2022-09-10T17:34:08.689068Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def word_probability_to_predict_df(text_to_word_probability, id):\n    len_word = len(text_to_word_probability)\n    word_predict = text_to_word_probability.argmax(-1)\n    word_score   = text_to_word_probability.max(-1)\n    predict_df = []\n\n    t = 0\n    while 1:\n        if word_predict[t] not in [\n            discourse_marker_to_label['O'],\n            discourse_marker_to_label['PAD'],\n        ]:\n            start = t\n            b_marker_label = word_predict[t]\n        else:\n            t = t+1\n            if t== len_word-1: break\n            continue\n\n        t = t+1\n        if t== len_word-1: break\n\n        #----\n        if   label_to_discourse_marker[b_marker_label][0]=='B':\n            i_marker_label = b_marker_label+1\n        elif label_to_discourse_marker[b_marker_label][0]=='I':\n            i_marker_label = b_marker_label\n        else:\n            raise NotImplementedError\n\n        while 1:\n            #print(t)\n            if (word_predict[t] != i_marker_label) or (t ==len_word-1):\n                end = t\n                prediction_string = ' '.join([str(i) for i in range(start,end)]) #np.arange(start,end).tolist()\n                discourse_type = label_to_discourse_marker[b_marker_label][2:]\n                discourse_score = word_score[start:end].tolist()\n                predict_df.append((id, discourse_type, prediction_string, str(discourse_score)))\n                #print(predict_df[-1])\n                break\n            else:\n                t = t+1\n                continue\n        if t== len_word-1: break\n\n    predict_df = pd.DataFrame(predict_df, columns=['id', 'class', 'predictionstring', 'score'])\n    return predict_df","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:08.695529Z","iopub.execute_input":"2022-09-10T17:34:08.695900Z","iopub.status.idle":"2022-09-10T17:34:08.712378Z","shell.execute_reply.started":"2022-09-10T17:34:08.695855Z","shell.execute_reply":"2022-09-10T17:34:08.711510Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def do_threshold(submit_df, use=['length','probability']):\n    df = submit_df.copy()\n    df = df.fillna('')\n\n    if 'length' in use:\n        df['l'] = df.predictionstring.apply(lambda x: len(x.split()))\n        for key, value in length_threshold.items():\n            #value=3\n            index = df.loc[df['class'] == key].query('l<%d'%value).index\n            df.drop(index, inplace=True)\n\n    if 'probability' in use:\n        df['s'] = df.score.apply(lambda x: np.mean(eval(x)))\n        for key, value in probability_threshold.items():\n            index = df.loc[df['class'] == key].query('s<%f'%value).index\n            df.drop(index, inplace=True)\n\n    df = df[['id', 'class', 'predictionstring']]\n    return df","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:08.716629Z","iopub.execute_input":"2022-09-10T17:34:08.717145Z","iopub.status.idle":"2022-09-10T17:34:08.729912Z","shell.execute_reply.started":"2022-09-10T17:34:08.717103Z","shell.execute_reply":"2022-09-10T17:34:08.729084Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class FeedbackModel(tez.Model):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        config = AutoConfig.from_pretrained(model_name)\n\n        hidden_dropout_prob: float = 0.1\n        layer_norm_eps: float = 1e-7\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = AutoModel.from_config(config)\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        logits = self.output(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:08.734150Z","iopub.execute_input":"2022-09-10T17:34:08.734452Z","iopub.status.idle":"2022-09-10T17:34:08.752949Z","shell.execute_reply.started":"2022-09-10T17:34:08.734380Z","shell.execute_reply":"2022-09-10T17:34:08.752094Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def jn(pst, start, end):\n    return \" \".join([str(x) for x in pst[start:end]])\n\ndef link_evidence(oof):\n    thresh = 1\n    idu = oof['id'].unique()\n    idc = idu[1]\n    eoof = oof[oof['class'] == \"Evidence\"]\n    neoof = oof[oof['class'] != \"Evidence\"]\n    for thresh2 in range(26, 27, 1):\n        retval = []\n        for idv in idu:\n            for c in ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n                      'Counterclaim', 'Rebuttal']:\n                q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n                if len(q) == 0:\n                    continue\n                pst = []\n                for i, r in q.iterrows():\n                    pst = pst + [-1] + [int(x) for x in r['predictionstring'].split()] # -1作为分割合并的间隔\n                start = 1\n                end = 1\n                for i in range(2, len(pst)):\n                    cur = pst[i]\n                    end = i\n                    # if pst[start] == 205:\n                    #   print(cur, pst[start], cur - pst[start])\n                    # Evidence的情况下：会一直延续到cur==-1的情况，然后判断下一个token是不是和之前的token仍旧满足26的最大距离要求，\n                    # 如果满足就相连，否则就append到retval里\n                    if (cur == -1 and c != 'Evidence') or ((cur == -1) and (\n                            (pst[i + 1] > pst[end - 1] + thresh) or (pst[i + 1] - pst[start] > thresh2))): #\n                        retval.append((idv, c, jn(pst, start, end)))\n                        start = i + 1\n                v = (idv, c, jn(pst, start, end + 1))\n                # print(v)\n                retval.append(v)\n        roof = pd.DataFrame(retval, columns=['id', 'class', 'predictionstring'])\n        roof = roof.merge(neoof, how='outer')\n        return roof","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:08.757309Z","iopub.execute_input":"2022-09-10T17:34:08.757550Z","iopub.status.idle":"2022-09-10T17:34:08.768496Z","shell.execute_reply.started":"2022-09-10T17:34:08.757519Z","shell.execute_reply":"2022-09-10T17:34:08.767579Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def run_submit():\n    results = []\n    all_model_idx = 0\n\n    for model_type, model_grp in model_grps.items():\n\n        print(f'{model_type} loaded ok.\\n')\n\n        arch = model_grp['arch']\n        num_models = len(model_grp['checkpoints'])\n\n        tokenizer = AutoTokenizer.from_pretrained(arch)\n\n        split_texts = np.array_split(df_text[['id', 'text']].values, 2)\n\n        tokenized_text_ = Parallel(n_jobs=2, backend=\"multiprocessing\")(\n            delayed(tokenize_data)(text, tokenizer, model_grp['max_length']) for text in split_texts\n        )\n\n        tokenized_text = []\n\n        for t in tokenized_text_:\n            tokenized_text.extend(t)\n\n        del tokenized_text_\n\n        collate_func = Collate(tokenizer)\n        valid_dataset = FeedbackDataset(tokenized_text)\n        valid_loader = DataLoader(\n            valid_dataset,\n            sampler=SequentialSampler(valid_dataset),\n            batch_size=model_grp['batch_size'],  # 4, #\n            drop_last=False,\n            num_workers=2,  # 0, #\n            pin_memory=False,\n            collate_fn = collate_func,\n        )\n\n        # start here !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1\n        results_n = {\n            'id': [],\n            'token_mask': [],\n            'token_offset': [],\n            'probability': [],\n        }\n       \n        for model_idx, (checkpoint, name) in enumerate(model_grp['checkpoints']):\n            net_template = FeedbackModel(arch, 15)\n            net_template.load_state_dict(torch.load(checkpoint))\n            net_template.cuda()\n\n            print('load ok : [%d] %s' % (all_model_idx, name))\n            all_model_idx += 1\n\n            T = 0\n            start_timer = timer()\n            for t, batch in enumerate(valid_loader):\n                batch_size = len(batch['index'])\n                token_mask = batch['token_mask']\n                token_id = batch['token_id']\n#                 print(\"data_shape\", token_id.shape)\n                token_mask = token_mask.cuda()\n                token_id = token_id.cuda()\n\n                net_template.eval()\n                with torch.no_grad():\n                    with amp.autocast(enabled=is_amp):\n                        probability = data_parallel(net_template, (token_id, token_mask))\n                        # probability = net[n](token_id, token_mask)\n\n                        if model_idx == 0:\n                            results_n['probability'].append(\n                                ((probability / num_models) * 255).byte().data.cpu().numpy())\n                            results_n['token_offset'] += [eval(x) for x in batch['token_offset']]\n                            results_n['token_mask'].append(token_mask.bool().data.cpu().numpy())\n                            results_n['id'] += batch['id']\n\n                        else:\n                            results_n['probability'][t] = results_n['probability'][t] + (\n                                        (probability / num_models) * 255).byte().data.cpu().numpy()\n\n                        T += batch_size\n\n                print('\\r\\t%d/%d  %s' % (T, len(valid_dataset), time_to_str(timer() - start_timer, 'sec')), end='',\n                      flush=True)\n#                 del token_id,token_mask\n            del net_template,token_id,token_mask\n            \n            gc.collect()\n\n            torch.cuda.empty_cache()\n            print('')\n       \n        # ----------------------------\n        del valid_dataset, valid_loader, tokenized_text\n        gc.collect()\n        \n        print('')\n\n        prob_list = []\n        for b_idx in range(len(results_n[\"probability\"])):\n            prob_list.extend(np.split(results_n[\"probability\"][b_idx], len(results_n[\"probability\"][b_idx])))\n\n        results.append({\n            'probability': prob_list,\n            'token_offset': np.array(results_n['token_offset'], object)\n            # 'token_mask'] = np.concatenate(results['token_mask'])\n            # 'id'] = np.array(results['id' ])\n        })\n    # ------------------------------------------------------------------------\n    # results: [num_net, [prob list]]\n    num_net = len(model_grps.keys())\n\n    submit_df = []\n    for i in range(num_valid):\n        d = df_text.iloc[i]\n        id = d.id\n        text = d.text\n        word, word_offset = text_to_word(text)\n        # print(i,id[i], len(text), len(word))\n\n        # ensemble -----\n        token_to_text_probability = np.full((len(text), num_discourse_marker), 0, np.float32)\n        for j in range(num_net):\n            p = results[j]['probability'][i][0][1:] / 255 # due to np.split adding another dim [0]\n            for t, (start, end) in enumerate(results[j]['token_offset'][i]):\n                if t == max_length - 1: break  # assume max_length, else use token_mask to get length\n                token_to_text_probability[start:end] += (p[t])  # **0.5\n        token_to_text_probability = token_to_text_probability / num_net\n        # ensemble -----\n\n        text_to_word_probability = np.full((len(word), num_discourse_marker), 0, np.float32)\n        for t, (start, end) in enumerate(word_offset):\n            text_to_word_probability[t] = token_to_text_probability[start:end].mean(0)\n\n        predict_df = word_probability_to_predict_df(text_to_word_probability, id)\n        submit_df.append(predict_df)\n        # print('\\r preparing submit_df :', i, id, len(text), len(word), end ='', flush=True)\n    print('')\n\n    # ----------------------------------------\n    submit_df = pd.concat(submit_df).reset_index(drop=True)\n    submit_df = do_threshold(submit_df, use=['length', 'probability'])\n    submit_df = link_evidence(submit_df)\n    submit_df.to_csv('submission.csv', index=False)\n\n    print('----')\n    for t in range(3): print(submit_df.iloc[t], '\\n')\n    print('submission ok!----')","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:08.770116Z","iopub.execute_input":"2022-09-10T17:34:08.770419Z","iopub.status.idle":"2022-09-10T17:34:08.794826Z","shell.execute_reply.started":"2022-09-10T17:34:08.770386Z","shell.execute_reply":"2022-09-10T17:34:08.794090Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:08.796450Z","iopub.execute_input":"2022-09-10T17:34:08.796671Z","iopub.status.idle":"2022-09-10T17:34:08.814781Z","shell.execute_reply.started":"2022-09-10T17:34:08.796631Z","shell.execute_reply":"2022-09-10T17:34:08.813870Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"run_submit()","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:34:08.815871Z","iopub.execute_input":"2022-09-10T17:34:08.816503Z","iopub.status.idle":"2022-09-10T17:34:37.805930Z","shell.execute_reply.started":"2022-09-10T17:34:08.816468Z","shell.execute_reply":"2022-09-10T17:34:37.805151Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"deberta loaded ok.\n\nload ok : [0] deberta_fold_0\n\t5/5   0 min 02 sec\n\n\n----\nid                                                       18409261F5C2\nclass                                                        Evidence\npredictionstring    162 163 164 165 166 167 168 169 170 171 172 17...\nName: 0, dtype: object \n\nid                                                       18409261F5C2\nclass                                                        Evidence\npredictionstring    441 442 443 444 445 446 447 448 449 450 451 45...\nName: 1, dtype: object \n\nid                                                       18409261F5C2\nclass                                                        Evidence\npredictionstring    739 740 741 742 743 744 745 746 747 748 749 75...\nName: 2, dtype: object \n\nsubmission ok!----\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The code below was taken from kaggle user tianmin. <a href=\"https://www.kaggle.com/code/tianmin/visualization-of-essay-dataset\">This</a> is the link to his notebook.","metadata":{}},{"cell_type":"markdown","source":"\nSo let's use some colors to make each part more outstanding.\n\nLet's give each discourse type a color, as shown below:\n","metadata":{}},{"cell_type":"markdown","source":"<h3 style=\"color:rgb(255, 102, 204);\">Lead</h3>\n<h3 style=\"color:rgb(0, 0, 102);\">Position</h3>\n<h3 style=\"color:rgb(51, 102, 255);\">Claim</h3>\n<h3 style=\"color:rgb(153, 102, 51);\">Counterclaim</h3>\n<h3 style=\"color:rgb(102, 204, 255);\">Rebuttal</h3>\n<h3 style=\"color:rgb(0, 0, 0);\">Evidence</h3>\n<h3 style=\"color:rgb(51, 51, 153);\">Concluding Statement</h3>","metadata":{}},{"cell_type":"code","source":"# iterate each discourse type and print it in the color as we wished\ndef color_article(id):\n    train = pd.read_csv('../input/feedback-prize-2021/train.csv')\n    sample = train.loc[train['id'] == id]\n    for i in range(sample.shape[0]):\n        text = sample['discourse_text'].iloc[i]\n        discourse = sample['discourse_type'].iloc[i]\n        if discourse == 'Lead':\n            color = '(255, 102, 204)'\n        elif discourse == 'Position':\n            color = '(0, 0, 102)'\n        elif discourse == 'Claim':\n            color = '(51, 102, 255)'\n        elif discourse == 'Counterclaim':\n            color = '(153, 102, 51)'\n        elif discourse == 'Rebuttal':\n            color = '(102, 204, 255)'\n        elif discourse == 'Evidence':\n            color = '(0, 0, 0)'\n        elif discourse == 'Concluding Statement':\n            color = '(51, 51, 153)'\n\n        sample_html_text = '<p style=\"color:rgb' + str(color) + ';\">' + str(text) +'  (' +str(discourse)+') ' + '</p>'\n        display(HTML(sample_html_text))\ncolor_article(\"007ACE74B050\")\n","metadata":{"execution":{"iopub.status.busy":"2022-09-10T17:39:37.567578Z","iopub.execute_input":"2022-09-10T17:39:37.567862Z","iopub.status.idle":"2022-09-10T17:39:38.393631Z","shell.execute_reply.started":"2022-09-10T17:39:37.567830Z","shell.execute_reply":"2022-09-10T17:39:38.392795Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<p style=\"color:rgb(255, 102, 204);\">Hi, i'm Isaac, i'm going to be writing about how this face on Mars is a natural landform or if there is life on Mars that made it. The story is about how NASA took a picture of Mars and a face was seen on the planet. NASA doesn't know if the landform was created by life on Mars, or if it is just a natural landform.   (Lead) </p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<p style=\"color:rgb(0, 0, 102);\">On my perspective, I think that the face is a natural landform because I dont think that there is any life on Mars. In these next few paragraphs, I'll be talking about how I think that is is a natural landform   (Position) </p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<p style=\"color:rgb(51, 102, 255);\">I think that the face is a natural landform because there is no life on Mars that we have descovered yet   (Claim) </p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<p style=\"color:rgb(0, 0, 0);\">If life was on Mars, we would know by now. The reason why I think it is a natural landform because, nobody live on Mars in order to create the figure. It says in paragraph 9, \"It's not easy to target Cydonia,\" in which he is saying that its not easy to know if it is a natural landform at this point. In all that they're saying, its probably a natural landform.   (Evidence) </p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<p style=\"color:rgb(153, 102, 51);\">People thought that the face was formed by alieans because they thought that there was life on Mars.  (Counterclaim) </p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<p style=\"color:rgb(102, 204, 255);\">though some say that life on Mars does exist, I think that there is no life on Mars.   (Rebuttal) </p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<p style=\"color:rgb(0, 0, 0);\">It says in paragraph 7, on April 5, 1998, Mars Global Surveyor flew over Cydonia for the first time. Michael Malin took a picture of Mars with his Orbiter Camera, that the face was a natural landform.   (Evidence) </p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<p style=\"color:rgb(153, 102, 51);\">Everyone who thought it was made by alieans even though it wasn't, was not satisfied. I think they were not satisfied because they have thought since 1976 that it was really formed by alieans.   (Counterclaim) </p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<p style=\"color:rgb(51, 51, 153);\">Though people were not satified about how the landform was a natural landform, in all, we new that alieans did not form the face. I would like to know how the landform was formed. we know now that life on Mars doesn't exist.  (Concluding Statement) </p>"},"metadata":{}}]}]}